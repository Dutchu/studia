{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ćwiczenie 2\n",
    "**Aproksymacja linii metodą największego spadku (Gradient Descent) w perceptronie**\n",
    "\n",
    "W tym ćwiczeniu wykorzystujemy perceptron do aproksymacji prostej opisującej zależność między zmiennymi, przy użyciu algorytmu gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cel\n",
    "Celem ćwiczenia jest wyznaczenie prostej aproksymującej zbiór punktów wygenerowanych na płaszczyźnie kartezjańskiej.\n",
    "W zadaniu stosujemy metodę największego spadku (gradient descent) do minimalizacji błędu średniokwadratowego (MSE), co pozwala na iteracyjne dostosowywanie wag modelu. Dzięki temu perceptron uczy się aproksymować rzeczywisty przebieg danych."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wstęp teoretyczny\n",
    "W ćwiczeniu wykorzystujemy dwie kluczowe koncepcje:\n",
    "\n",
    "1. **Perceptron**\n",
    "   To najprostsza sieć neuronowa, która dla danych wejściowych przyjmuje postać funkcji liniowej:\n",
    "$$\n",
    "   h(x) = a \\cdot x + b\n",
    "$$\n",
    "   gdzie \\(a\\) jest współczynnikiem kierunkowym, a \\(b\\) wyrazem wolnym.\n",
    "\n",
    "2. **Metoda największego spadku (Gradient Descent)**\n",
    "   Algorytm optymalizacji, który pozwala znaleźć minimum funkcji celu, iteracyjnie aktualizując parametry modelu w kierunku przeciwnym do gradientu.\n",
    "   Funkcją celu w naszym przypadku jest błąd średniokwadratowy (MSE):\n",
    "$$\n",
    "   E = \\frac{1}{M} \\sum_{i=1}^{M} \\left(y_i - h(x_i)\\right)^2\n",
    "$$\n",
    "   Parametry \\(a\\) i \\(b\\) są aktualizowane według reguły:\n",
    "$$\n",
    "   w_k = w_{k-1} + \\eta \\cdot \\frac{\\partial E}{\\partial w}\\Bigr|_{w_{k-1}}\n",
    "$$\n",
    "   gdzie $\\eta$ to współczynnik uczenia (learning rate).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:49.689905900Z",
     "start_time": "2025-03-01T05:05:49.675707500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:49.718222100Z",
     "start_time": "2025-03-01T05:05:49.682387400Z"
    }
   },
   "outputs": [],
   "source": [
    "# Ustawienia:\n",
    "np.random.seed(42)  # dla powtarzalności wyników\n",
    "M = 15  # liczba punktów pomiarowych\n",
    "eta = 0.01  # współczynnik uczenia (można eksperymentować, np. 0.01 lub 0.05)\n",
    "delta_threshold = 0.01  # kryterium zakończenia iteracji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:49.719220500Z",
     "start_time": "2025-03-01T05:05:49.696212200Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generowanie danych\n",
    "# Przyjmujemy, że prawdziwa prosta to: y = 2.0*x + 1.0, a do tego dodajemy niewielki szum\n",
    "x_data = np.linspace(0, 10, M)\n",
    "noise = np.random.randn(M) * 1.0  # szum o niewielkiej wariancji\n",
    "y_data = 2.0 * x_data + 1.0 + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:49.729445800Z",
     "start_time": "2025-03-01T05:05:49.710703100Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inicjalizacja parametrów\n",
    "a = 0.0  # początkowa wartość współczynnika kierunkowego\n",
    "b = 0.0  # początkowa wartość wyrazu wolnego\n",
    "\n",
    "# Funkcja aproksymująca:\n",
    "def h(x, a, b):\n",
    "    return a * x + b\n",
    "\n",
    "# Funkcja licząca błąd średniokwadratowy:\n",
    "def mse(y_true, y_pred):\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "# Obliczamy gradienty błędu względem a i b\n",
    "def compute_gradients(x, y, a, b):\n",
    "    M = len(y)\n",
    "    # Predykcje\n",
    "    y_pred = h(x, a, b)\n",
    "    # Obliczamy błędy\n",
    "    error = y - y_pred\n",
    "    # Gradienty – pochodne cząstkowe błędu E względem a i b\n",
    "    grad_a = (2/M) * np.sum(error * x)   # pochodna względem a\n",
    "    grad_b = (2/M) * np.sum(error)         # pochodna względem b\n",
    "    return grad_a, grad_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:49.743702Z",
     "start_time": "2025-03-01T05:05:49.728447700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Proces uczenia metodą największego spadku\n",
    "max_iterations = 10000  # maksymalna liczba iteracji, zabezpieczenie przed nieskończoną pętlą\n",
    "E_prev = mse(y_data, h(x_data, a, b))\n",
    "iteration = 0\n",
    "\n",
    "while iteration < max_iterations:\n",
    "    # Obliczenie gradientów\n",
    "    grad_a, grad_b = compute_gradients(x_data, y_data, a, b)\n",
    "\n",
    "    # Aktualizacja parametrów\n",
    "    a = a + eta * grad_a\n",
    "    b = b + eta * grad_b\n",
    "\n",
    "    # Obliczenie nowego błędu\n",
    "    E_curr = mse(y_data, h(x_data, a, b))\n",
    "\n",
    "    # Obliczenie różnicy błędów\n",
    "    delta = abs(E_curr - E_prev)\n",
    "\n",
    "    # Dla celów diagnostycznych można wypisać postęp co kilka iteracji\n",
    "    if iteration % 100 == 0:\n",
    "        print(f\"Iteracja {iteration:4d}: E = {E_curr:.4f}, delta = {delta:.4f}, a = {a:.4f}, b = {b:.4f}\")\n",
    "\n",
    "    # Warunek zakończenia pętli\n",
    "    if delta <= delta_threshold:\n",
    "        print(f\"Zakończono iteracje po {iteration} krokach, delta = {delta:.4f}\")\n",
    "        break\n",
    "\n",
    "    # Przygotowanie do kolejnej iteracji\n",
    "    E_prev = E_curr\n",
    "    iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:49.817432500Z",
     "start_time": "2025-03-01T05:05:49.744710300Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wyniki końcowe\n",
    "print(\"\\nOstateczne parametry prostej:\")\n",
    "print(f\"a (współczynnik kierunkowy) = {a:.4f}\")\n",
    "print(f\"b (wyraz wolny) = {b:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:05:50.125907Z",
     "start_time": "2025-03-01T05:05:49.759772Z"
    }
   },
   "outputs": [],
   "source": [
    "# Wizualizacja wyników\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(x_data, y_data, color='blue', label='Punkty pomiarowe')\n",
    "# Generujemy wartości dla linii aproksymującej\n",
    "x_line = np.linspace(x_data.min()-1, x_data.max()+1, 100)\n",
    "y_line = h(x_line, a, b)\n",
    "plt.plot(x_line, y_line, color='red', linewidth=2, label='Prosta aproksymująca')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Aproksymacja linii metodą gradient descent')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wnioski\n",
    "Ćwiczenie wykazało, że metoda gradient descent jest skutecznym narzędziem do optymalizacji parametrów modelu liniowego.\n",
    "Dzięki iteracyjnemu dostosowywaniu wag perceptronu udało się znaleźć prostą, która minimalizuje błąd średniokwadratowy między wartościami rzeczywistymi a modelowanymi.\n",
    "Wyniki eksperymentu potwierdzają zasadność teoretycznych założeń i praktyczną użyteczność podejścia, co czyni je wartościowym narzędziem w analizie i aproksymacji danych.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (psio)",
   "language": "python",
   "name": "psio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
