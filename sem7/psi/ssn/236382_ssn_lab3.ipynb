{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Ćwiczenie 3\n",
    "W niniejszym ćwiczeniu uczymy jednowarstwową sieć neuronową do klasyfikacji zwierząt na podstawie wybranych cech (np. liczba nóg, środowisko życia, zdolność latania itp.). Sieć otrzymuje na wejściu wektor cech, a na wyjściu zwraca wektor klas (np. ssak, ptak, ryba). Uczenie przebiega w trybie online – w każdej iteracji wybierany jest losowy przykład, a wagi sieci są korygowane na podstawie różnicy między oczekiwanym a rzeczywistym wyjściem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Cel\n",
    "Celem ćwiczenia jest:\n",
    "1. Zapoznanie się z budową i działaniem jednowarstwowej sieci neuronowej z funkcją aktywacji sigmoidalnej.\n",
    "2. Zaimplementowanie algorytmu uczenia sieci metodą gradientu prostego (regułą delta).\n",
    "3. Zbadanie, w jaki sposób dobór współczynnika uczenia i liczby epok wpływa na skuteczność klasyfikacji."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wstęp teoretyczny\n",
    "W rozważanym modelu sieci jednowarstwowej każdemu neuronu towarzyszy funkcja aktywacji w postaci funkcji sigmoidalnej (logistycznej). Dla sygnału wejściowego $$\\mathbf{p}$$ i wag $$\\mathbf{W}$$ wyjście neuronu oblicza się następująco:\n",
    "\n",
    "$$\n",
    "\\mathrm{net} = \\mathbf{W} \\cdot \\mathbf{p},\n",
    "$$\n",
    "\n",
    "a następnie przeprowadza przez funkcję aktywacji:\n",
    "\n",
    "$$\n",
    "\\sigma(\\mathrm{net}) = \\frac{1}{1 + e^{-\\mathrm{net}}}.\n",
    "$$\n",
    "\n",
    "W przypadku wielu neuronów wyjściowych, każdy z nich ma własny zestaw wag, a powyższe równanie liczone jest dla każdej kolumny macierzy $$\\mathbf{W}$$ (lub wiersza, w zależności od przyjętej konwencji).\n",
    "\n",
    "Uczenie sieci polega na wielokrotnym prezentowaniu przykładów uczących – w każdej iteracji wybierany jest jeden z nich, obliczane jest wyjście sieci $$\\mathbf{out}$$, a następnie błąd:\n",
    "\n",
    "$$\n",
    "\\mathbf{e} = \\mathbf{t} - \\mathbf{out},\n",
    "$$\n",
    "\n",
    "gdzie $$\\mathbf{t}$$ to wektor oczekiwany (tzw. wektor nauczyciela). Aby dostosować wagi, obliczamy wektor $$\\boldsymbol{\\delta}$$ z uwzględnieniem pochodnej sigmoidy:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\delta} = \\mathbf{e} \\odot \\mathbf{out} \\odot \\bigl(1 - \\mathbf{out}\\bigr),\n",
    "$$\n",
    "\n",
    "gdzie $$\\odot$$ oznacza iloczyn Skalarny (element-wise). Aktualizacja wag przebiega zgodnie z regułą:\n",
    "\n",
    "$$\n",
    "\\Delta \\mathbf{W} = \\eta \\cdot \\boldsymbol{\\delta} \\cdot \\mathbf{p}^T,\n",
    "$$\n",
    "\n",
    "a następnie:\n",
    "\n",
    "$$\n",
    "\\mathbf{W} \\leftarrow \\mathbf{W} + \\Delta \\mathbf{W},\n",
    "$$\n",
    "\n",
    "gdzie $$\\eta$$ jest współczynnikiem uczenia (learning rate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:32:00.004447300Z",
     "start_time": "2025-03-01T05:31:59.986249800Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Definicja funkcji uczącej sieć z zapisem historii błędu\n",
    "def train_single_layer_with_error(P, T, W, n, eta=0.1):\n",
    "    \"\"\"\n",
    "    Funkcja ucząca jednowarstwową sieć neuronową z aktywacją sigmoidalną\n",
    "    oraz zapisywaniem błędu średniokwadratowego (MSE) w każdej epoce.\n",
    "\n",
    "    Parametry:\n",
    "      P  : macierz wejść (liczba_cech x liczba_przykładów)\n",
    "      T  : macierz wyjść oczekiwanych (liczba_wyjść x liczba_przykładów)\n",
    "      W  : macierz wag (liczba_wyjść x liczba_cech)\n",
    "      n  : liczba epok (iteracji przez zbiór uczący)\n",
    "      eta: współczynnik uczenia\n",
    "\n",
    "    Zwraca:\n",
    "      W : zaktualizowaną macierz wag\n",
    "      errors : lista wartości MSE obliczanych na koniec każdej epoki\n",
    "    \"\"\"\n",
    "    num_samples = P.shape[1]\n",
    "    errors = []\n",
    "\n",
    "    for epoch in range(n):\n",
    "        epoch_errors = []\n",
    "        # Losowa kolejność przykładów w każdej epoce\n",
    "        indices = np.random.permutation(num_samples)\n",
    "        for i in indices:\n",
    "            # Pobieramy przykładowe wejście i wektor nauczyciela (kolumnowo)\n",
    "            p = P[:, i].reshape(-1, 1)\n",
    "            t = T[:, i].reshape(-1, 1)\n",
    "\n",
    "            # Propagacja w przód: obliczenie net i aktywacji sigmoidalnej\n",
    "            net = np.dot(W, p)\n",
    "            out = 1 / (1 + np.exp(-net))\n",
    "\n",
    "            # Obliczenie błędu\n",
    "            e = t - out\n",
    "\n",
    "            # Delta – błąd uwzględniający pochodną funkcji sigmoidalnej\n",
    "            delta = e * out * (1 - out)\n",
    "\n",
    "            # Aktualizacja wag\n",
    "            W += eta * np.dot(delta, p.T)\n",
    "\n",
    "            # Zapis błędu dla tego przykładu\n",
    "            epoch_errors.append(np.mean(e**2))\n",
    "\n",
    "        # Średni błąd epoki\n",
    "        errors.append(np.mean(epoch_errors))\n",
    "\n",
    "    return W, errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:32:00.004953200Z",
     "start_time": "2025-03-01T05:31:59.996692800Z"
    }
   },
   "outputs": [],
   "source": [
    "# Przykładowa definicja macierzy wejść P (każda kolumna to inny przykład)\n",
    "# Poniżej 5 przykładowych kolumn (zwierząt), a 4 cechy w wierszach.\n",
    "P = np.array([\n",
    "    [4.0,  0.0,  1.0,  4.0,  1.0],  # liczba nóg\n",
    "    [1.0,  1.0,  0.0,  1.0,  0.0],  # czy żyje w wodzie\n",
    "    [0.0,  1.0,  1.0,  0.0,  1.0],  # czy potrafi latać\n",
    "    [1.0,  3.5,  0.0,  2.0,  0.0],  # przykładowa inna cecha\n",
    "], dtype=float)\n",
    "\n",
    "# Przykładowa definicja macierzy nauczyciela T (każda kolumna odpowiada tej samej kolumnie w P)\n",
    "# Załóżmy, że mamy 3 klasy: [ssak, ptak, ryba].\n",
    "T = np.array([\n",
    "    [1, 0, 0, 1, 0],  # ssak\n",
    "    [0, 1, 0, 0, 1],  # ptak\n",
    "    [0, 0, 1, 0, 0],  # ryba\n",
    "], dtype=float)\n",
    "\n",
    "# Inicjalizacja wag (losowo), wymiary: (liczba_wyjść x liczba_wejść)\n",
    "# Tutaj liczba_wyjść = 3, liczba_wejść = 4.\n",
    "W_poczatkowe = np.random.randn(T.shape[0], P.shape[0])\n",
    "\n",
    "# Parametry uczenia\n",
    "n = 10       # liczba epok\n",
    "eta = 0.1    # współczynnik uczenia\n",
    "\n",
    "# Funkcja 'train_single_layer' powinna być zaimportowana lub zdefiniowana w tym samym pliku/skrypcie\n",
    "W_nauczone = train_single_layer(P, T, W_poczatkowe, n, eta)\n",
    "\n",
    "print(\"Wyuczone wagi:\\n\", W_nauczone)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:32:00.037732800Z",
     "start_time": "2025-03-01T05:32:00.004447300Z"
    }
   },
   "outputs": [],
   "source": [
    "W_nauczone = train_single_layer(P, T, W_poczatkowe, n=10, eta=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:32:00.038732700Z",
     "start_time": "2025-03-01T05:32:00.016564400Z"
    }
   },
   "outputs": [],
   "source": [
    "p_new = np.array([[4], [1], [0], [1]])  # teraz ma kształt (4,1)\n",
    "net_new = np.dot(W_nauczone, p_new)\n",
    "out_new = 1 / (1 + np.exp(-net_new))\n",
    "print(\"Wyjście sieci dla nowego przykładu:\", out_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:32:00.039733200Z",
     "start_time": "2025-03-01T05:32:00.024750600Z"
    }
   },
   "outputs": [],
   "source": [
    "# Parametry uczenia\n",
    "n = 10       # liczba epok\n",
    "eta = 0.1    # współczynnik uczenia\n",
    "\n",
    "# Trening sieci oraz zapis historii błędu\n",
    "W_nauczone, error_history = train_single_layer_with_error(P, T, W_poczatkowe, n, eta)\n",
    "\n",
    "print(\"Wyuczone wagi:\\n\", W_nauczone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T05:32:00.820011100Z",
     "start_time": "2025-03-01T05:32:00.036730600Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Wykres MSE w kolejnych epokach\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.lineplot(x=np.arange(1, n+1), y=error_history, marker=\"o\")\n",
    "plt.title(\"Wykres błędu średniokwadratowego (MSE) w kolejnych epokach\")\n",
    "plt.xlabel(\"Epoka\")\n",
    "plt.ylabel(\"Błąd średniokwadratowy (MSE)\")\n",
    "plt.show()\n",
    "\n",
    "# Heatmapa macierzy wag nauczonej\n",
    "plt.figure(figsize=(6, 4))\n",
    "ax = sns.heatmap(W_nauczone, annot=True, cmap=\"coolwarm\", fmt=\".2f\")\n",
    "plt.title(\"Macierz wag nauczonej sieci\")\n",
    "plt.xlabel(\"Wejścia (cechy)\")\n",
    "plt.ylabel(\"Neurony wyjściowe (klasy)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Wnioski\n",
    "1. Sieć jednowarstwowa z funkcją sigmoidalną może skutecznie rozróżniać przykłady należące do różnych klas, jeśli dane są odpowiednio dobrane i wystarczająco zróżnicowane.\n",
    "2. Zbyt mała wartość\n",
    "    $$\\eta$$\n",
    "   powoduje wolną zbieżność algorytmu, natomiast zbyt duża może prowadzić do oscylacji i utrudnić osiągnięcie minimum błędu.\n",
    "3. Liczba epok treningowych powinna być dobrana tak, aby sieć mogła wystarczająco często zobaczyć każdy przykład i dostosować do niego wagi.\n",
    "4. Dodanie wiersza jedynek (bias) do macierzy wejściowej może znacząco poprawić skuteczność klasyfikacji, ponieważ pozwala sieci na przesunięcie hiperpowierzchni decyzyjnej w przestrzeni cech.\n",
    "5. Podejście online, z losowym doborem przykładów, może zapobiec lokalnym minimom i przyspieszyć uczenie w porównaniu do metody batch."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (psio)",
   "language": "python",
   "name": "psio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
