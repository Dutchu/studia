{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Sztuczne sieci neuronowe: ćwiczenie 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Opis i cel ćwiczenia\n",
    "Celem ćwiczenia jest zapoznanie się z:\n",
    "1. **Modelem perceptronu** i zagadnieniem klasyfikacji punktów w przestrzeni 2D z użyciem środowiska Python.\n",
    "2. **Sieciami jednokierunkowymi (feed-forward)** i ich zastosowaniem do aproksymacji funkcji przy użyciu różnych algorytmów uczenia:\n",
    "   - train_gd (gradient descent),\n",
    "   - train_gdm (gradient descent with momentum),\n",
    "   - train_gda (gradient descent with adaptive learning rate).\n",
    "3. **Wpływem liczby próbek uczących** (mały vs duży zbiór) na proces uczenia i dokładność sieci.\n",
    "4. **Siecią rekurencyjną Elmana** – zapoznanie się ze strukturą, działaniem i wpływem liczby neuronów w warstwie ukrytej na dokładność odtwarzania sygnału."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Wstęp teoretyczny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.1. Perceptron\n",
    "Perceptron jest jedną z najprostszych sieci neuronowych – zawiera warstwę wejściową (złożoną z neuronów wejściowych – nieprzetwarzających) oraz pojedynczy neuron wyjściowy z odpowiednią funkcją aktywacji (np. funkcją skoku). Perceptron dokonuje **liniowej separacji** zbioru uczącego w przestrzeni wejściowej. Jeśli punkty uczące są nieliniowo separowalne, perceptron nie jest w stanie wyuczyć się prawidłowego rozdzielenia tych punktów."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.2. Sieć jednokierunkowa (feed-forward) do aproksymacji funkcji\n",
    "Wielowarstwowe sieci perceptronowe (MLP – Multi-Layer Perceptron) stosowane są m.in. do aproksymacji funkcji. Typowo składają się one z:\n",
    "- warstwy wejściowej,\n",
    "- jednej lub więcej warstw ukrytych z nieliniową funkcją aktywacji (np. sigmoidalną),\n",
    "- warstwy wyjściowej (np. z liniową funkcją aktywacji).\n",
    "\n",
    "W niniejszym ćwiczeniu wykorzystano bibliotekę **neurolab** w języku Python, która udostępnia różne metody uczenia (m.in. `train_gd`, `train_gdm`, `train_gda`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Algorytmy uczenia:\n",
    "- **train_gd** – zwykła metoda spadku wzdłuż gradientu (ang. *gradient descent*).\n",
    "- **train_gdm** – metoda spadku wzdłuż gradientu z momentum (ang. *gradient descent with momentum*), gdzie dodatkowo uwzględniany jest człon bezwładności (momentu).\n",
    "- **train_gda** – metoda spadku wzdłuż gradientu z adaptacyjnym doborem współczynnika uczenia (ang. *gradient descent with adaptive learning rate*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 2.3. Sieć Elmana\n",
    "Sieć Elmana jest przykładem **sieci rekurencyjnej**, w której wprowadza się sprzężenie zwrotne z warstwy ukrytej do tzw. warstwy kontekstowej. Warstwa kontekstowa przechowuje stany neuronów ukrytych z poprzedniej chwili czasowej (poprzez opóźnienie jednostkowe). Dzięki temu sieć może „pamiętać” poprzednie stany i uwzględniać je w aktualnych obliczeniach.\n",
    "\n",
    "Strukturalnie, sygnały z warstwy ukrytej wracają do wejścia (lub do warstwy poprzedniej) przez blok opóźniający `z^-1`, co pozwala analizować ciągi czasowe czy zadania, w których potrzebna jest informacja o poprzednich stanach.\n",
    "\n",
    "Schemat sieci Elmana można opisać następująco:\n",
    "- Wejścia: \\( x_0(k), x_1(k), ..., x_N(k) \\),\n",
    "- Kontekst (przechowuje poprzednie wyjścia warstwy ukrytej): \\( v_1(k-1), ..., v_K(k-1) \\),\n",
    "- Warstwa ukryta o sygnałach wyjściowych \\( v_i(k) \\),\n",
    "- Warstwa wyjściowa generująca sygnały \\( y_i(k) \\).\n",
    "\n",
    "Wagi aktualizowane są metodą gradientową uwzględniającą rekurencję."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Przebieg zadania\n",
    "\n",
    "### 3.1. Zadanie 1: „Program 1 – modelowanie działania perceptronu”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:37.828823Z",
     "start_time": "2025-03-01T06:17:37.746117500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:37.829821100Z",
     "start_time": "2025-03-01T06:17:37.754154900Z"
    }
   },
   "outputs": [],
   "source": [
    "# Przykładowe punkty uczące\n",
    "# Każdy wiersz to [x1, x2], label\n",
    "train_data = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 1, 0],\n",
    "    [1, 0, 0],\n",
    "    [1, 1, 1]\n",
    "    # Można dodać punkty nieliniowo separowalne\n",
    "])\n",
    "\n",
    "X = train_data[:, 0:2]\n",
    "y = train_data[:, 2]\n",
    "\n",
    "# Inicjalizacja wag\n",
    "w = np.random.rand(3)  # w0 = bias, w1, w2\n",
    "\n",
    "def perceptron_predict(x, w):\n",
    "    # x = [1, x1, x2] (z dodaną jedynką na początku)\n",
    "    return 1 if np.dot(w, x) >= 0 else 0\n",
    "\n",
    "def train_perceptron(X, y, w, epochs=10, eta=0.1):\n",
    "    # X – macierz Nx2, y – etykiety, w – wektor wag\n",
    "    X_bias = np.c_[np.ones(X.shape[0]), X]  # dodanie kolumny jedynek\n",
    "    for _ in range(epochs):\n",
    "        for i in range(len(X)):\n",
    "            y_hat = perceptron_predict(X_bias[i], w)\n",
    "            e = y[i] - y_hat\n",
    "            # aktualizacja wag\n",
    "            w += eta * e * X_bias[i]\n",
    "    return w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Proces uczenia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:37.874516200Z",
     "start_time": "2025-03-01T06:17:37.764225500Z"
    }
   },
   "outputs": [],
   "source": [
    "w_learned = train_perceptron(X, y, w, epochs=20, eta=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Testowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:37.875544600Z",
     "start_time": "2025-03-01T06:17:37.773255400Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(X)):\n",
    "    x_bias = np.array([1, X[i,0], X[i,1]])\n",
    "    print(f\"Punkt {X[i]} -> przewidywana klasa = {perceptron_predict(x_bias, w_learned)}, rzeczywista = {y[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Obserwacje:\n",
    "\n",
    "Dla punktów liniowo separowalnych perceptron zbiega do rozwiązania, które klasyfikuje zbiór uczący poprawnie.\n",
    "Dodanie punktów niespełniających kryterium liniowej separowalności powoduje, że perceptron nie jest w stanie rozdzielić wszystkich punktów poprawnie. W praktyce wagi będą się wahać i sieć nie osiągnie 100% poprawności."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.2\n",
    "### Zadanie 2: „Program 2. Modelowanie zagadnienia dopasowania za pomocą sieci typu feed-forward”\n",
    "Cel: Zamodelować uczenie sieci MLP aproksymującej dwie funkcje (zaimplementowane w kodzie) i obliczyć błąd średniokwadratowy (MSE) dla różnych:\n",
    "\n",
    "- Liczb neuronów w warstwie ukrytej: 3, 5, 10, 20, 30, 50\n",
    "- Metod uczenia: train_gd, train_gdm, train_gda\n",
    "\n",
    "Następnie powtórzyć eksperyment z większą liczbą próbek (np. 130 zamiast 30)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:50.849304700Z",
     "start_time": "2025-03-01T06:17:37.788720200Z"
    }
   },
   "outputs": [],
   "source": [
    "import neurolab as nl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Przykładowa funkcja do aproksymacji:\n",
    "def f1(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def f2(x):\n",
    "    return np.cos(x)\n",
    "\n",
    "# Generowanie danych\n",
    "x_min, x_max = -2*np.pi, 2*np.pi\n",
    "num_samples = 30  # lub 130 w drugim podejściu\n",
    "x = np.linspace(x_min, x_max, num_samples).reshape(num_samples,1)\n",
    "y1 = f1(x).reshape(num_samples,1)\n",
    "y2 = f2(x).reshape(num_samples,1)\n",
    "\n",
    "# Można wybrać jedną z funkcji:\n",
    "input_data = x\n",
    "target_data = y1  # lub y2\n",
    "\n",
    "# Parametry\n",
    "hidden_neurons_list = [3, 5, 10, 20, 30, 50]\n",
    "train_methods = ['train_gd', 'train_gdm', 'train_gda']\n",
    "\n",
    "results = {}\n",
    "\n",
    "for tm in train_methods:\n",
    "    results[tm] = []\n",
    "    for hn in hidden_neurons_list:\n",
    "        # Tworzymy sieć MLP: 1 wejście, 1 warstwa ukryta (hn neuronów), 1 wyjście\n",
    "        net = nl.net.newff([[x_min, x_max]], [hn, 1], [nl.trans.TanSig(), nl.trans.PureLin()])\n",
    "        mse = np.mean((output - target_data)**2)\n",
    "        results[tm].append(mse)\n",
    "\n",
    "        # Ustawiamy metodę uczenia\n",
    "        net.trainf = getattr(nl.train, tm)\n",
    "\n",
    "        # Trening\n",
    "        error = net.train(input_data, target_data, epochs=500, show=100, goal=0.001)\n",
    "\n",
    "        # Symulacja\n",
    "        output = net.sim(input_data)\n",
    "\n",
    "        # Wyliczenie MSE\n",
    "        mse = np.mean((output - target_data)**2)\n",
    "        results[tm].append(mse)\n",
    "\n",
    "        # Można zapisać lub wyświetlić wykres\n",
    "        # plt.plot(error)\n",
    "        # plt.show()\n",
    "\n",
    "# Wyświetlenie podsumowania\n",
    "for tm in train_methods:\n",
    "    print(f\"Metoda uczenia: {tm}\")\n",
    "    for hn, mse_val in zip(hidden_neurons_list, results[tm]):\n",
    "        print(f\"  Liczba neuronów: {hn}, MSE = {mse_val}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:50.858794400Z",
     "start_time": "2025-03-01T06:17:50.853322400Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display, Latex\n",
    "# Budujemy tabelę w LaTeX\n",
    "table_lines = []\n",
    "table_lines.append(r\"\\begin{table}[h]\")\n",
    "table_lines.append(r\"\\centering\")\n",
    "table_lines.append(r\"\\begin{tabular}{l|cccccc}\")\n",
    "table_lines.append(r\"\\hline\")\n",
    "table_lines.append(r\"\\textbf{Liczba neuronów} & \" + \" & \".join(str(hn) for hn in hidden_neurons_list) + r\" \\\\\")\n",
    "table_lines.append(r\"\\hline\")\n",
    "\n",
    "for tm in train_methods:\n",
    "    # np. \"MSE (train_gd)\"\n",
    "    row_label = f\"MSE ({tm})\"\n",
    "    # Formatowanie liczb (np. 5 miejsc po przecinku)\n",
    "    row_values = [f\"{val:.5f}\" for val in results[tm]]\n",
    "    row_str = row_label + \" & \" + \" & \".join(row_values) + r\" \\\\\"\n",
    "    table_lines.append(row_str)\n",
    "\n",
    "table_lines.append(r\"\\hline\")\n",
    "table_lines.append(r\"\\end{tabular}\")\n",
    "table_lines.append(r\"\\caption{Porównanie wartości MSE dla różnych metod uczenia i liczby neuronów}\")\n",
    "table_lines.append(r\"\\label{tab:mse_auto}\")\n",
    "table_lines.append(r\"\\end{table}\")\n",
    "\n",
    "# Łączymy linie w jeden string\n",
    "table_code = \"\\n\".join(table_lines)\n",
    "\n",
    "display(Latex(table_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3.3. Sieci rekurencyjne na przykładzie sieci Elmana (Zadanie 3)\n",
    "W zadaniu wykorzystujemy przykładowy Program 3 (poniżej fragment kodu), w którym testujemy zdolność sieci Elmana do odtwarzania sygnału prostokątnego o wartościach [1, 2]. Sprawdzamy wpływ liczby neuronów w warstwie ukrytej na dokładność odwzorowania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:53.834930400Z",
     "start_time": "2025-03-01T06:17:50.864317600Z"
    }
   },
   "outputs": [],
   "source": [
    "import neurolab as nl\n",
    "import numpy as np\n",
    "import pylab as pl\n",
    "\n",
    "# Create train samples\n",
    "i1 = np.sin(np.arange(0, 20))\n",
    "i2 = np.sin(np.arange(0, 20)) * 2\n",
    "t1 = np.ones([1, 20])\n",
    "t2 = np.ones([1, 20]) * 2\n",
    "\n",
    "input = np.array([i1, i2, i1, i2]).reshape(20 * 4, 1)\n",
    "target = np.array([t1, t2, t1, t2]).reshape(20 * 4, 1)\n",
    "\n",
    "# Create network with 2 layers\n",
    "# newelm - tworzy sieć Elmana\n",
    "net = nl.net.newelm([[-2, 2]], [20, 1], [nl.trans.TanSig(), nl.trans.PureLin()])\n",
    "\n",
    "# Inicjalizacja\n",
    "net.layers[0].initf = nl.init.InitRand([-0.1, 0.1], 'wb')\n",
    "net.layers[1].initf = nl.init.InitRand([-0.1, 0.1], 'wb')\n",
    "net.init()\n",
    "\n",
    "# Trening\n",
    "error = net.train(input, target, epochs=500, show=100, goal=0.01)\n",
    "\n",
    "# Symulacja\n",
    "output = net.sim(input)\n",
    "\n",
    "# Rysunki\n",
    "pl.subplot(211)\n",
    "pl.plot(error)\n",
    "pl.xlabel('Epoch number')\n",
    "pl.ylabel('Train error (default MSE)')\n",
    "pl.subplot(212)\n",
    "pl.plot(target.reshape(80))\n",
    "pl.plot(output.reshape(80))\n",
    "pl.legend(['train target', 'net output'])\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T06:17:59.021723Z",
     "start_time": "2025-03-01T06:17:53.838939700Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lista różnych rozmiarów warstwy ukrytej\n",
    "hidden_neurons_list = [3, 5, 20, 50, 100]\n",
    "\n",
    "# Tablica do przechowywania wyników (MSE)\n",
    "mse_results = []\n",
    "\n",
    "# Pętla po różnych liczbach neuronów\n",
    "for hn in hidden_neurons_list:\n",
    "    # Tworzymy sieć Elmana (newelm)\n",
    "    # Zakładamy wejście np. w przedziale [-2,2]; dostosuj w razie potrzeby\n",
    "    net = nl.net.newelm([[-2, 2]], [hn, 1], [nl.trans.TanSig(), nl.trans.PureLin()])\n",
    "\n",
    "    # Inicjalizacja wag\n",
    "    net.init()\n",
    "\n",
    "    # Trening (np. 500 epok, goal=0.01)\n",
    "    # W praktyce parametry ustalasz w zależności od zadania\n",
    "    error = net.train(input_data, target_data, epochs=500, show=100, goal=0.01)\n",
    "\n",
    "    # Symulacja\n",
    "    output = net.sim(input_data)\n",
    "\n",
    "    # Obliczenie błędu średniokwadratowego\n",
    "    mse = np.mean((output - target_data) ** 2)\n",
    "\n",
    "    # Dodaj wynik do listy\n",
    "    mse_results.append(mse)\n",
    "\n",
    "# --- Generowanie tabeli w LaTeX ---\n",
    "print(r\"\\begin{table}[h]\")\n",
    "print(r\"\\centering\")\n",
    "# Liczba kolumn = 1 (opis) + tyle kolumn, ile rozmiarów warstwy ukrytej\n",
    "print(r\"\\begin{tabular}{l|\" + \"c\"*len(hidden_neurons_list) + \"}\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "# Nagłówek: \"Liczba neuronów w warstwie ukrytej\"\n",
    "print(\"Liczba neuronów w warstwie ukrytej & \" + \" & \".join(str(hn) for hn in hidden_neurons_list) + r\" \\\\\")\n",
    "print(r\"\\hline\")\n",
    "\n",
    "# Drugi wiersz: \"Średniokwadratowy błąd\"\n",
    "# Możemy sformatować liczby, np. do 4 miejsc po przecinku\n",
    "mse_str = \" & \".join(f\"{m:.4f}\" for m in mse_results)\n",
    "print(\"Średniokwadratowy błąd & \" + mse_str + r\" \\\\\")\n",
    "\n",
    "print(r\"\\hline\")\n",
    "print(r\"\\end{tabular}\")\n",
    "print(r\"\\caption{Wpływ liczby neuronów w warstwie ukrytej na MSE (sieć Elmana)}\")\n",
    "print(r\"\\label{tab:elman_mse}\")\n",
    "print(r\"\\end{table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.3.2 Obserwacje\n",
    "Wraz ze wzrostem liczby neuronów w warstwie ukrytej sieć Elmana zwykle lepiej (z mniejszym błędem) odwzorowuje sygnał, jednak nadmierne zwiększanie liczby neuronów może prowadzić do wydłużenia czasu treningu lub przeuczenia w innych zastosowaniach.\n",
    "Dla prostego sygnału prostokątnego [1, 2] już niewielka liczba neuronów (np. 5–20) może zapewnić zadowalającą dokładność.\n",
    "Optymalny rozmiar warstwy ukrytej zależy od charakteru sygnału i liczby dostępnych próbek.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# 4. Wnioski końcowe\n",
    "Perceptron skutecznie rozdziela dane, o ile są liniowo separowalne. Przy nieliniowo separowalnych nie osiąga zadowalających wyników – należy wtedy stosować bardziej złożone sieci.\n",
    "Sieci MLP (feed-forward) z jedną warstwą ukrytą mogą aproksymować różne funkcje, a jakość aproksymacji zależy od:\n",
    "Liczby neuronów w warstwie ukrytej (zbyt mała – niedouczenie, zbyt duża – możliwe przeuczenie).\n",
    "Metody uczenia (train_gd, train_gdm, train_gda) – dodanie momentu lub adaptacyjnego współczynnika uczenia może przyspieszyć i ustabilizować proces uczenia.\n",
    "Wielkości zbioru uczącego – większy zbiór zwykle poprawia uogólnianie.\n",
    "Sieć Elmana dzięki rekurencji potrafi „pamiętać” poprzednie stany i lepiej odwzorowywać sygnały czasowe (np. sygnał prostokątny). Liczba neuronów w warstwie ukrytej znacząco wpływa na dokładność i stabilność uczenia.\n",
    "Istnieje kompromis pomiędzy złożonością sieci (liczbą neuronów) a ryzykiem przeuczenia i czasem uczenia. W praktyce liczba neuronów w warstwie ukrytej dobierana jest eksperymentalnie, w oparciu o wiedzę o zadaniu i testy empiryczne.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (psio)",
   "language": "python",
   "name": "psio"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
