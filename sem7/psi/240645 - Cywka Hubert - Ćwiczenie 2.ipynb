{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44136fba05b42522",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Podstawy sztucznej inteligencji\n",
    "## Ćwiczenie 2 - Wykorzystanie Sztucznych Sieci Neuronowych do rozwiązywania zagadnienia aproksymacji przebiegów w czasie.\n",
    "### 240645 - Hubert Cywka"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b91b0840a53e50",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 1. Cel ćwiczenia\n",
    "Celem pierwszego ćwiczenia jest rozwiązania zagadnienia aproksymacji przebiegów w czasie za pomocą pojedynczego neuronu. Nauka neuronu odbędzie się przy pomocy metody największego spadku."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08a922a4dc975ca",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 2. Wstęp\n",
    "Aby zapewnić spójne zachowanie aplikacji przy każdym jej uruchomieniu, ustawiłem stałą wartość ziarna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12853f8eea88502",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:46:05.738579Z",
     "start_time": "2024-12-13T16:46:05.734400Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c7d4f99a538cb",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Model neuronu można opisać wzorem $y = f(w * x + b)$, gdzie $f$ to liniowa funkcja aktywacji. Dla każdego punktu danych neuron oblicza błąd względem wartości oczekiwanej $y$ stosując wyrażenie $e = y - \\hat{y}$, gdzie $\\hat{y}$ to wartość wyjściowa obliczona przez neuron, a $y$ to wartość oczekiwana. Podczas procesu uczenia neuron iteracyjnie minimalizuje błąd średniokwadratowy ($MSE$):\n",
    " \n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} \\left( y_i - \\hat{y}_i \\right)^2$$\n",
    "\n",
    "Korekta wag $w$ i przesunięcia $b$ odbywa się zgodnie z następującymi wzorami: $w_k = w_{k-1} + η * e * x$ oraz $b_k = b_{k-1} * η * e$, gdzie $η$ to współczynnik uczenia. Proces uczenia trwa, dopóki zmiana $MSE$ między kolejnymi iteracjami nie spadnie poniżej określonego progu wynoszącego $0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647a359bc5f4840b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 3. Rozwiązanie\n",
    "Algorytm realizujący uczenie perceptronu zawarty został w całości w klasie `ApproximationPerceptron` zdefiniowanej poniżej:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ef76d37d99ab2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:46:05.766739Z",
     "start_time": "2024-12-13T16:46:05.761202Z"
    }
   },
   "outputs": [],
   "source": [
    "class ApproximationPerceptron:\n",
    "    def __init__(self, learning_rate):\n",
    "        self.weight = np.random.random() / 2 \n",
    "        self.bias = np.random.random() / 2\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def activation(self, x):\n",
    "        return x\n",
    "\n",
    "    def predict(self, x):\n",
    "        z = np.dot(x, self.weight) + self.bias\n",
    "        return self.activation(z)\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        previous_mse = float('inf')\n",
    "        \n",
    "        while True:\n",
    "            total_error = 0 \n",
    "\n",
    "            for i in range(len(x)):\n",
    "                y_pred = self.predict(x[i])\n",
    "                error = y[i] - y_pred\n",
    "\n",
    "                self.weight += self.learning_rate * error * x[i]\n",
    "                self.bias += self.learning_rate * error\n",
    "                total_error += error ** 2\n",
    "\n",
    "            mse = total_error / len(x)\n",
    "            delta = abs(mse - previous_mse)\n",
    "            previous_mse = mse\n",
    "\n",
    "            if delta <= 0.01:\n",
    "                break\n",
    "\n",
    "        return self.weight, self.bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce49c09b000b584",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Zdefiniowałem funkcje pomocnicze do generowania danych treningowych oraz danych testowych:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9352cac2df436f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:46:05.789623Z",
     "start_time": "2024-12-13T16:46:05.785194Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_points_for_function(f, x_range, n_points, noise):\n",
    "    x_values = np.random.uniform(x_range[0], x_range[1], n_points)\n",
    "    y_values = [f(x) + np.random.uniform(-noise, noise) for x in x_values]\n",
    "    return np.array(x_values), np.array(y_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4a5a7f5c99ff0d",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Wartości $x$ zostały wybrane z przedziału $(0, 1)$. Dla każdej wartości x została wyznaczona wartość $y$, taka, że: $y = 3 * x + 2.5 + a$, gdzie $a$ jest losową wartością z przedziału $(-0.25, 0.25)$. Wygenerowanych zostało 1000 punktów do danych treningowych i 100 punktów do danych testowych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b7fa618398774e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-13T16:46:05.932277Z",
     "start_time": "2024-12-13T16:46:05.791166Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_approximation(x, y, perceptron):\n",
    "    plt.scatter(x, y, color='blue', label='Dane rzeczywiste')\n",
    "\n",
    "    x_vals = np.linspace(min(x), max(x), 100)\n",
    "    y_vals = perceptron.weight * x_vals + perceptron.bias\n",
    "    plt.plot(x_vals, y_vals, color='red', label='Linia aproksymacji')\n",
    "\n",
    "    plt.title(\"Aproksymacja z użyciem perceptronu\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "def calculate_relative_error(y_true, y_pred):\n",
    "    relative_errors = np.abs((y_true - y_pred) / y_true)\n",
    "    mean_relative_error = np.mean(relative_errors)\n",
    "    return mean_relative_error\n",
    "\n",
    "func = lambda x: 3 * x + 2.5\n",
    "x_range = (0, 1)\n",
    "noise = 0.25\n",
    "\n",
    "train_data_x, train_data_y = generate_points_for_function(func, x_range, 1000, noise)\n",
    "test_data_x, test_data_y = generate_points_for_function(func, x_range, 100, noise)\n",
    "\n",
    "perceptron = ApproximationPerceptron(0.01)\n",
    "perceptron.fit(train_data_x, train_data_y)\n",
    "\n",
    "plot_approximation(test_data_x, test_data_y, perceptron)\n",
    "test_y_pred = [perceptron.predict(x) for x in train_data_x]\n",
    "relative_error = calculate_relative_error(np.array(train_data_y), np.array(test_y_pred))\n",
    "print(\"Średni błąd względny:\", relative_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704eae947ee6ddb2",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 4. Wnioski\n",
    "Ćwiczenie zostało zrealizowane pomyślnie, perceptron poprawnie zaproksymował zdefinowaną wcześniej funkcję $y = 3 * x + 2.5$. Już po trzeciej iteracji nauki udało się osiągnąć średni błąd względny na poziomie 3.49%. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
