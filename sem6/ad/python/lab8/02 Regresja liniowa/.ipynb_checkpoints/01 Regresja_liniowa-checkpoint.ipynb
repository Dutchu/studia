{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresja liniowa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresja to proces nadzorowanego uczenia maszynowego, podobny do klasyfikacji, jednak polegający na prognozowaniu ciągłych wartości, a nie etykiet.\n",
    "Jeżeli celem analizy jest prognozowanie liczb, należy stosować regresję.\n",
    "\n",
    "Jak się okazuje, wiele modeli klasyfikacyjnych udostępnianych przez bibliotekę sklearn można wykorzystywać do rozwiązywania problemów regresyjnych. Udostępniany jest ten sam interfejs API składający się z metod fit, score i predict. Tak samo jest w przypadku wzmacniających bibliotek nowej generacji XGBoost i LightGBM.\n",
    "\n",
    "Jakość modeli regresyjnych, pomimo ich podobieństw do modeli klasyfikacyjnych i hiperparametrów, ocenia się z użyciem innych wskaźników."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regresyjny model odniesienia stanowi bazę, z którą porównywane są inne modele. Domyślnym wynikiem metody score z biblioteki sklearn jest współczynnik determinacji (oznaczany jako r2 lub R2). Jest to stopień, w jakim zmienność danych wejściowych przekłada się na zmienność prognozowanych wyników. Zazwyczaj współczynnik ten przyjmuje wartości z przedziału od 0 do 1, choć w przypadku szczególnie złych modeli może być liczbą ujemną."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O0beFDfiWD97"
   },
   "source": [
    "## Wprowadzenie\n",
    "\n",
    "Zdefiniujmy:\n",
    "- $X_1, X_2, ..., X_n$ - zmienne niezależne (nasze dane do modelu)\n",
    "- $Y$ - zmienna docelowa\n",
    "- $y_{true}$ - wartość rzeczywista\n",
    "- $y_{pred}$ - wartość przewidziana przez model\n",
    "- $w_0, w_1,...,w_n$ - wagi do modelu (podlegaja uczeniu)\n",
    "\n",
    "W tym modelu zakłada się. ze wartość przewidywana $y_{pred}$ może być liniową kombinacją zmiennych niezależnych. Ogólna postac modelu:\n",
    "\n",
    ">  $$y_{pred}(W, X) = w_0 + w_1X_1 + ... + w_nX_n$$\n",
    ">  $$Y = XW$$\n",
    "\n",
    "gdzie:\n",
    ">  $X = \\begin{pmatrix} 1 & X_1 & X_2 & \\dots & X_n\\end{pmatrix}$,  $W = \\begin{pmatrix} w_{0} \\\\  w_{1}  \\\\ \\dots \\\\ w_{n}\\end{pmatrix}$\n",
    "\n",
    "Wprowadźmy oznaczenia:\n",
    "- $w = (w_1,...w_n)$ jako `coef_`\n",
    "- $w_0$ jako `intercept_`\n",
    "\n",
    "Regresja Liniowa polega na takim dopasowaniu wag $w_0, w_1,...,w_n$ by zminimalizować funkcję kosztu(cost function):\n",
    ">$$||XW-Y||_{2}^{2} \\rightarrow min$$  \n",
    "\n",
    "Przykład w $R^2$:  \n",
    "$Y=w_0 + w_1X_1$\n",
    "\n",
    "Przykład w $R^3$:  \n",
    "$Y=w_0 + w_1X_1 + w_2X_2$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g9TFJqBl6zsj"
   },
   "source": [
    "**Podstawowe założenia regresji liniowej**\n",
    "\n",
    "* mamy zależność liniową\n",
    "* wariancja reszt jest taka sama dla wszystkich obserwacji\n",
    "* brak współliniowości (żaden z predyktorów nie stanowi kombinacji liniowej innych predyktorów)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "peQ6zK3Vf6bL"
   },
   "source": [
    "## Przykładowe dane\n",
    "### Wygenerowanie danych\n",
    "\n",
    "Przykład w $R^2$:  \n",
    "$Y=w_0 + w_1X_1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "hkcj5Yb_VhrK",
    "outputId": "6961388f-04ad-4930-c673-10c29a3f4a7f"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(10)\n",
    "\n",
    "X = np.arange(0, 50, 0.5)\n",
    "noise = 10 * np.random.randn(100)\n",
    "y = 2 * X + 100 + noise\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "print('Rozmiar X:',X.shape)\n",
    "print('Rozmiar y:', y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mFgAtdiUknXb"
   },
   "source": [
    "### Podział danych na zbiór treningowy i testowy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "dOEYo3hCkq_z",
    "outputId": "fc56459f-c960-431c-c634-2f7143781949"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oopABUY3f93i"
   },
   "source": [
    "### Wizualizacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 356
    },
    "colab_type": "code",
    "id": "c74espDVe-Wj",
    "outputId": "44a15d52-49c9-41a2-cacd-106ce822dce0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(X_train, y_train, c='b', label='Zbiór treningowy')\n",
    "plt.scatter(X_test, y_test, c='g', label='Zbiór testowy')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sNdzJ6cRpu6T"
   },
   "source": [
    "## Regresja liniowa - model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "CxMuN3E4fTWJ",
    "outputId": "c400edf1-a99c-436b-caa6-14c2daa4cbba"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "\n",
    "print(lin_reg.coef_)\n",
    "print(lin_reg.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFC7wSpmmbT1"
   },
   "source": [
    "Rozwiązaniem jest prosta o postaci:\n",
    "$Y=101.86 + 1.93\\cdot X_1$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aFC7wSpmmbT1"
   },
   "source": [
    "### Wizualizacja zbioru treningowego i dopasowanego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "Lsaj34safVTY",
    "outputId": "21cf81e1-3f92-442a-b3fb-2c5ef12a4299"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.title('Regresja Liniowa - jedna zmienna')\n",
    "plt.scatter(X_train, y_train, c='b', label='Zbiór treningowy')\n",
    "plt.plot(X, lin_reg.intercept_ + lin_reg.coef_[0] * X, c='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EpmSbc4Xm92m"
   },
   "source": [
    "###  Wizualizacja zbioru testowego i dopasowanego modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "id": "N00QTIDtnFCV",
    "outputId": "a0e4e415-0483-4d13-ffb9-be96b56e9c3f"
   },
   "outputs": [],
   "source": [
    "y_pred = lin_reg.predict(X_test)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.title('Regresja Liniowa - jedna zmienna')\n",
    "plt.scatter(X_test, y_test, c='g', label='Zbiór testowy')\n",
    "plt.plot(X, lin_reg.intercept_ + lin_reg.coef_[0] * X, c='red')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVT9rxbRj8za"
   },
   "source": [
    "###  Ocena modelu\n",
    "Metoda `score()` zwraca współczynnik determinacji $R^2$  naszej predykcji.\n",
    "\n",
    "Współczynnik determinacji jest zdefiniowany jako:\n",
    "\n",
    "$R^{2} =1 - \\frac{\\sum_{t=1}^{n}(y_{pred, t} - \\bar{y}_{true} )^2}{\\sum_{t=1}^{n}(y_{true, t} - \\bar{y}_{true} )^2}$\n",
    "\n",
    "Współczynnik determinacji jest miarą stopnia dopasowania modelu do próby. Dopasowanie modelu jest tym lepsze im wartość $R^2$ jest bliżej 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Bhxx8bLgj_0S",
    "outputId": "a4c821c6-6e06-40e9-c9f2-7d9a471cc819"
   },
   "outputs": [],
   "source": [
    "lin_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IQvG16titmEF"
   },
   "outputs": [],
   "source": [
    "lin_reg.score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d0uNxKGUjQlv"
   },
   "source": [
    "## Przykład złego zastosowania regresji liniowej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "colab_type": "code",
    "id": "ooqDZvsXiGkq",
    "outputId": "5e5e05cc-5b62-43a0-bc71-d1896695aeb9"
   },
   "outputs": [],
   "source": [
    "X = np.arange(-5, 5, 0.5)\n",
    "noise = 10 * np.random.randn(20)\n",
    "y = 2 * (X ** 2) + 4 + noise\n",
    "X = X.reshape(-1, 1)\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "\n",
    "print(lin_reg.coef_)\n",
    "print(lin_reg.intercept_)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.title('Regresja Liniowa - jedna zmienna')\n",
    "plt.plot(X, lin_reg.intercept_ + lin_reg.coef_[0] * X, c='red')\n",
    "plt.scatter(X, y)\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGb55kFe6MhN"
   },
   "source": [
    "# Regresja Wielomianowa - Polynomial Regression\n",
    "\n",
    "Powszechnym wzorcem stosowanym w uczeniu maszynowym jest użycie modeli liniowych wytrenowanych na nieliniowych funkcjach danych. Podejscie to utrzymuje szybkie działanie metod liniowych i zarazem umożliwia stosowanie metod liniowych dla znacznie szerszego zakresu danych.\n",
    "\n",
    "Przykładowo, prosta regresja liniowa może zostać zastosowana przez skonstruowanie cech wielomianowych do modelów nieliniowych.\n",
    "\n",
    "Rozważmy model regresji liniowej:\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2$$\n",
    "\n",
    "Dodając cechy wielomianowe otrzymujemy:\n",
    "\n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 + w_2 x_2 + w_3 x_1 x_2 + w_4 x_1^2 + w_5 x_2^2$$\n",
    "\n",
    "Wyobraźmy sobie teraz następujące podstawienie:\n",
    "\n",
    "$$z = [1, x_1, x_2, x_1 x_2, x_1^2, x_2^2]$$\n",
    "\n",
    "Wrzucając to z powrotem do naszego modelu dotrzymujemy prosty model liniowy:\n",
    "\n",
    "$$\\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2 + w_3 z_3 + w_4 z_4 + w_5 z_5$$\n",
    "\n",
    "To implikuje fakt, iż nasz model regresji wielomianowej znajduje się w klasie modeli liniowych i może być rozwiązany za pomocą tych samych technik. \n",
    "\n",
    "Rozważmy model regresji liniowej: \n",
    "\n",
    "$$\\hat{y}(w,x)=w_0+w_1x_1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.arange(-10, 10, 0.5)\n",
    "noise = 80 * np.random.randn(40)\n",
    "y = -X**3 + 10*X**2 - 2*X + 3 + noise\n",
    "X = X.reshape(40, 1)\n",
    "_ = plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regressor = LinearRegression()\n",
    "regressor.fit(X, y)\n",
    "y_pred = regressor.predict(X)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "_ = plt.plot(X, y_pred, c='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model jest niedouczony (underfitting). Potrzebujemy zwiekszyć złozoność modelu do naszego problemu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja cech wielomianowych - stopień 2\n",
    "\n",
    "Rozważmy model regresji liniowej:\n",
    "    \n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1$$\n",
    "\n",
    "Dodając cechy wielomianowe otrzymujemy:\n",
    "    \n",
    "$$\\hat{y}(w, x) = w_0 + w_1 x_1 +w_2x_1^2 $$\n",
    "\n",
    "Wyobraźmy sobie teraz nastepujące podstawienie:\n",
    "    \n",
    "$$z = [1, x_1, x_1^2]$$\n",
    "\n",
    "Wrzucając to z powrotem do naszego modelu otrzymujemy prosty model liniowy:\n",
    "    \n",
    "$$\\hat{y}(w, z) = w_0 + w_1 z_1 + w_2 z_2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolynomialFeatures**\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html\n",
    "\n",
    "Generacja cech wielomianowych.\n",
    "Wygeneruj nową macierz cech składającą się ze wszystkich kombinacji wielomianów cech o stopniu mniejszym lub równym określonemu stopniowi. \n",
    "\n",
    "Na przykład, jeśli próbka wejściowa jest dwuwymiarowa i ma postać $[a, b]$, cechy wielomianu stopnia 2 to $[1, a, b, a^2, ab, b^2]$.\n",
    "\n",
    "Jeśli próbka jest jednowymiarowa $[a]$, cechy wielomianu stopnia 2 to $[1,a,a^2]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "poly = PolynomialFeatures(degree=2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dopasowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_poly = LinearRegression()\n",
    "regressor_poly.fit(X_poly, y)\n",
    "\n",
    "y_pred = regressor_poly.predict(X_poly)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "_ = plt.plot(X, y_pred, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ekstrakcja cech wielomianowych - stopień 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_poly = poly.fit_transform(X)\n",
    "X_poly[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dopasowanie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regressor_poly_3 = LinearRegression()\n",
    "regressor_poly_3.fit(X_poly, y)\n",
    "\n",
    "y_pred = regressor_poly_3.predict(X_poly)\n",
    "\n",
    "plt.scatter(X, y)\n",
    "_ = plt.plot(X, y_pred, c='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inne metody regresji - przykłady"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import (\n",
    "    model_selection,\n",
    "    preprocessing,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cof = 5\n",
    "np.random.seed(0)\n",
    "X = np.arange(-10, 10, 0.5/cof)\n",
    "noise = 80 * np.random.randn(40*cof)\n",
    "y = -X**3 + 10*X**2 - 2*X + 3 + noise\n",
    "X = X.reshape(40*cof, 1)\n",
    "_ = plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja liniowa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "[lr.score(X_test, y_test),lr.coef_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='rbf')\n",
    "svr.fit(X_train, y_train)\n",
    "svr.score(X_test, y_test) #R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='linear')\n",
    "svr.fit(X_train, y_train)\n",
    "[svr.score(X_test, y_test),svr.coef_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "svr = SVR(kernel='poly')\n",
    "svr.fit(X_train, y_train)\n",
    "svr.score(X_test, y_test) #R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "knr = KNeighborsRegressor()\n",
    "knr.fit(X_train, y_train)\n",
    "knr.score(X_test, y_test) #R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "dtr = DecisionTreeRegressor(random_state=42)\n",
    "dtr.fit(X_train, y_train)\n",
    "dtr.score(X_test, y_test) #R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install dtreeviz\n",
    "# https://github.com/parrt/dtreeviz/blob/master/notebooks/dtreeviz_sklearn_visualisations.ipynb\n",
    "import dtreeviz.trees\n",
    "dtr3 = DecisionTreeRegressor(max_depth=2)\n",
    "dtr3.fit(X_train, y_train)\n",
    "viz_model = dtreeviz.model(\n",
    "    dtr3,\n",
    "    X_train=X_train,\n",
    "    y_train=y_train,\n",
    "    target_name=\"y\"\n",
    ")\n",
    "viz_model.view(scale=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydotplus\n",
    "from io import StringIO\n",
    "from sklearn.tree import export_graphviz\n",
    "dot_data = StringIO()\n",
    "export_graphviz(\n",
    "    dtr,\n",
    "    out_file=dot_data,\n",
    "    feature_names=['X'], # X.columns,\n",
    "    filled=True,\n",
    ")\n",
    "g = pydotplus.graph_from_dot_data(\n",
    "    dot_data.getvalue()\n",
    ")\n",
    "g.write_png(\"tree.png\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import (\n",
    "    RandomForestRegressor,\n",
    ")\n",
    "rfr = RandomForestRegressor(\n",
    "    random_state=42, n_estimators=100\n",
    ")\n",
    "rfr.fit(X_train, y_train)\n",
    "rfr.score(X_test, y_test) #R^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost (eXtreme Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgr = xgb.XGBRegressor(random_state=42, )\n",
    "xgr.fit(X_train, y_train)\n",
    "xgr.score(X_test, y_test) #R^2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "xgb.plot_importance(xgr, ax=ax) # tu tylko jedna cecha :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = xgr.predict(X_test)\n",
    "\n",
    "plt.scatter(X_test, y_test)\n",
    "_ = plt.scatter(X_test, y_pred, c='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ocena modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = xgr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Resztkowy histogram**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot((y_test-predictions),bins=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metryki oceny regresji\n",
    "\n",
    "Oto trzy typowe metryki oceny problemów z regresją:\n",
    "\n",
    "**Średni błąd bezwzględny** (MAE) to średnia wartości bezwzględnej błędów:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n",
    "\n",
    "**Średni błąd kwadratowy** (MSE) to średnia kwadratów błędów:\n",
    "\n",
    "$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n",
    "\n",
    "**Pierwiastek ze średniego błądu kwadratowego** (RMSE) to pierwiastek kwadratowy średniej kwadratów błędów:\n",
    "\n",
    "$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n",
    "\n",
    "Porównując te dane:\n",
    "\n",
    "- **MAE** jest najłatwiejszy do zrozumienia, ponieważ jest to średni błąd.\n",
    "- **MSE** jest bardziej popularne niż MAE, ponieważ MSE „karze” większe błędy, co zwykle jest przydatne w prawdziwym świecie.\n",
    "- **RMSE** jest jeszcze bardziej popularny niż MSE, ponieważ RMSE można interpretować w jednostkach „y”.\n",
    "\n",
    "Wszystkie te funkcje są **funkcjami strat**, ponieważ chcemy je zminimalizować."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('MAE:', metrics.mean_absolute_error(y_test, predictions))\n",
    "print('MSE:', metrics.mean_squared_error(y_test, predictions))\n",
    "print('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, predictions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresja logistyczna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Jedną z najpopularniejszych odmian analizy regresji jest regresja logistyczna. Najważniejszą cechą regresji logistycznej jest to, że zmienna zależna (wyjaśniana, przewidywana) jest zmienną dychotomiczną, czyli przyjmuje dwie wartości, najczęściej 0 i 1. \n",
    "\n",
    "Jest to zatem klasyfikacja, nie zaś \"regresja\" rozumiana jako szacowanie wartości :)\n",
    "\n",
    "W klasycznej analizy regresji - model liniowy analizowaliśmy zależność pomiędzy dwiema zmiennymi mierzonymi na skali ilościowej. Zastosowanie modelu liniowego dla zmiennej zależnej mierzonej na skali dychotomicznej dałoby błędną interpretację, ponieważ model taki zakłada występowanie wartości poniżej 0 lub powyżej 1, a w przypadku zmiennej dychotomicznej nie mamy takich przypadków, coś może wystąpić bądź nie. \n",
    "\n",
    "Dlatego też zastosowanie klasycznej regresji liniowej dla zmiennych dychotomicznych jest nieodpowiednim podejściem analitycznym.\n",
    "\n",
    "Więcej, np.: https://asystaekspercka.pl/regresja-logistyczna.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bibliografia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Matt Harrison, Uczenie maszynowe w Pythonie (Leksykon kieszonkowy), Helion\n",
    "* Materiały internetowe, kursy i szkolenia\n",
    "\n",
    "Warto zajrzeć: https://machinelearningmastery.com/calculate-feature-importance-with-python/ \n",
    "\n",
    "oraz portal Kaggle"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "04_regresja_liniowa.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
